{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load the data\n",
    "df_train = pd.read_csv('./NLP_dataset/train.csv')\n",
    "df_test = pd.read_csv('./NLP_dataset/test.csv')\n",
    "df_submission = pd.read_csv('./NLP_dataset/submission_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>helpfulness_cat</th>\n",
       "      <th>imdb_user_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>It is hard to find such delightful and adorabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>They don't make films like this faded, hauntin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I first viewed this movie in 1924 at age 6 yrs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I doubt that I'd ever seen anything resembling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I was shocked to find myself riveted to this m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10750</th>\n",
       "      <td>0.0</td>\n",
       "      <td>The makers of this movie really touched a sore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10751</th>\n",
       "      <td>0.0</td>\n",
       "      <td>I Care A Lot is an exhilarating black comedy w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10752</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Really loved this. This film is masterful in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10753</th>\n",
       "      <td>0.0</td>\n",
       "      <td>The story, direction and acting across the boa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10754</th>\n",
       "      <td>0.0</td>\n",
       "      <td>This movie ruled! It had such a unique premise...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10755 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       helpfulness_cat                                   imdb_user_review\n",
       "0                  1.0  It is hard to find such delightful and adorabl...\n",
       "1                  1.0  They don't make films like this faded, hauntin...\n",
       "2                  1.0  I first viewed this movie in 1924 at age 6 yrs...\n",
       "3                  1.0  I doubt that I'd ever seen anything resembling...\n",
       "4                  1.0  I was shocked to find myself riveted to this m...\n",
       "...                ...                                                ...\n",
       "10750              0.0  The makers of this movie really touched a sore...\n",
       "10751              0.0  I Care A Lot is an exhilarating black comedy w...\n",
       "10752              0.0  Really loved this. This film is masterful in t...\n",
       "10753              0.0  The story, direction and acting across the boa...\n",
       "10754              0.0  This movie ruled! It had such a unique premise...\n",
       "\n",
       "[10755 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>helpfulness_cat</th>\n",
       "      <th>imdb_user_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>It is hard to find such delightful and adorabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>They don't make films like this faded, hauntin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I first viewed this movie in 1924 at age 6 yrs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I doubt that I'd ever seen anything resembling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I was shocked to find myself riveted to this m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10745</th>\n",
       "      <td>1.0</td>\n",
       "      <td>This started of great and then half way throug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10746</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I felt bad at watching this movie. This is my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10747</th>\n",
       "      <td>1.0</td>\n",
       "      <td>The plot is so cheap. Characters are horrible ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10748</th>\n",
       "      <td>1.0</td>\n",
       "      <td>What a horrible movie. The only good thing abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10749</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I'll be sure to look the law up on legal guard...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8214 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       helpfulness_cat                                   imdb_user_review\n",
       "0                  1.0  It is hard to find such delightful and adorabl...\n",
       "1                  1.0  They don't make films like this faded, hauntin...\n",
       "2                  1.0  I first viewed this movie in 1924 at age 6 yrs...\n",
       "3                  1.0  I doubt that I'd ever seen anything resembling...\n",
       "4                  1.0  I was shocked to find myself riveted to this m...\n",
       "...                ...                                                ...\n",
       "10745              1.0  This started of great and then half way throug...\n",
       "10746              1.0  I felt bad at watching this movie. This is my ...\n",
       "10747              1.0  The plot is so cheap. Characters are horrible ...\n",
       "10748              1.0  What a horrible movie. The only good thing abo...\n",
       "10749              1.0  I'll be sure to look the law up on legal guard...\n",
       "\n",
       "[8214 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['helpfulness_cat']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>helpfulness_cat</th>\n",
       "      <th>imdb_user_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>The movie is best described as pathetic. It ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Well i was watching this movie for whatever re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>This is A Film of A Conservative Man Who Is St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>This is probably the cheesiest most melodramat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>This movie should have been destroyed not rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10750</th>\n",
       "      <td>0.0</td>\n",
       "      <td>The makers of this movie really touched a sore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10751</th>\n",
       "      <td>0.0</td>\n",
       "      <td>I Care A Lot is an exhilarating black comedy w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10752</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Really loved this. This film is masterful in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10753</th>\n",
       "      <td>0.0</td>\n",
       "      <td>The story, direction and acting across the boa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10754</th>\n",
       "      <td>0.0</td>\n",
       "      <td>This movie ruled! It had such a unique premise...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2541 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       helpfulness_cat                                   imdb_user_review\n",
       "5                  0.0  The movie is best described as pathetic. It ma...\n",
       "6                  0.0  Well i was watching this movie for whatever re...\n",
       "10                 0.0  This is A Film of A Conservative Man Who Is St...\n",
       "32                 0.0  This is probably the cheesiest most melodramat...\n",
       "33                 0.0  This movie should have been destroyed not rest...\n",
       "...                ...                                                ...\n",
       "10750              0.0  The makers of this movie really touched a sore...\n",
       "10751              0.0  I Care A Lot is an exhilarating black comedy w...\n",
       "10752              0.0  Really loved this. This film is masterful in t...\n",
       "10753              0.0  The story, direction and acting across the boa...\n",
       "10754              0.0  This movie ruled! It had such a unique premise...\n",
       "\n",
       "[2541 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['helpfulness_cat']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>imdb_user_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>610d01fe9a63eb113d2235ac</td>\n",
       "      <td>This is basically a German takeoff on a Sherlo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>610d01fe9a63eb113d224536</td>\n",
       "      <td>In January of 1924, director Erich von Strohei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>610d01fe9a63eb113d224d14</td>\n",
       "      <td>Silent movies are not for everyone. Neither ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>610d01fe9a63eb113d225f17</td>\n",
       "      <td>It so often happens that some films take the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>610d01fe9a63eb113d225f1a</td>\n",
       "      <td>I saw this film for the very first time last w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5066</th>\n",
       "      <td>610d02269a63eb113d5ecec2</td>\n",
       "      <td>Had to fast forward a few parts of the movie j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5067</th>\n",
       "      <td>610d02269a63eb113d5ecec4</td>\n",
       "      <td>This is movie is total garbage. If I could rat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5068</th>\n",
       "      <td>610d02269a63eb113d5ececb</td>\n",
       "      <td>Paints court appointed guardians as completely...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5069</th>\n",
       "      <td>610d02269a63eb113d5ed17e</td>\n",
       "      <td>Everyone knows how incredible and talented Ros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5070</th>\n",
       "      <td>610d02269a63eb113d5ed242</td>\n",
       "      <td>Terrific performances across the board. Genius...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5071 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           _id  \\\n",
       "0     610d01fe9a63eb113d2235ac   \n",
       "1     610d01fe9a63eb113d224536   \n",
       "2     610d01fe9a63eb113d224d14   \n",
       "3     610d01fe9a63eb113d225f17   \n",
       "4     610d01fe9a63eb113d225f1a   \n",
       "...                        ...   \n",
       "5066  610d02269a63eb113d5ecec2   \n",
       "5067  610d02269a63eb113d5ecec4   \n",
       "5068  610d02269a63eb113d5ececb   \n",
       "5069  610d02269a63eb113d5ed17e   \n",
       "5070  610d02269a63eb113d5ed242   \n",
       "\n",
       "                                       imdb_user_review  \n",
       "0     This is basically a German takeoff on a Sherlo...  \n",
       "1     In January of 1924, director Erich von Strohei...  \n",
       "2     Silent movies are not for everyone. Neither ar...  \n",
       "3     It so often happens that some films take the l...  \n",
       "4     I saw this film for the very first time last w...  \n",
       "...                                                 ...  \n",
       "5066  Had to fast forward a few parts of the movie j...  \n",
       "5067  This is movie is total garbage. If I could rat...  \n",
       "5068  Paints court appointed guardians as completely...  \n",
       "5069  Everyone knows how incredible and talented Ros...  \n",
       "5070  Terrific performances across the board. Genius...  \n",
       "\n",
       "[5071 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>helpfulness_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [_id, helpfulness_cat]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use any classifier or combination of classifiers, any combination or selection of features, and either supervised, semi-supervised, or even transfer learning approaches\n",
    "- BoW vectors, TFIDF vectors, vectors achieved with embedding algorithms, topic-to-document probabilities. \n",
    "- In terms of estimators, you can use logistic regression, Naive Bayes Classifier, or any other estimator you think is appropriate given the nature of the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import spacy\n",
    "from rich.console import Console\n",
    "from rich.table import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdr = \"/Users/timliu/Documents/GitHub/NLP-orgs-markets/sampleData/commencementSpeeches/corpus\" # 注意這邊是從github上面clone下來的 有很多text file\n",
    "# in_fs = glob.glob(os.path.join(fdr, \"*.txt\"))\n",
    "# # load the transcripts into a dictionary\n",
    "# speeches = {}\n",
    "# for in_f in in_fs:\n",
    "#     with open(in_f, \"r\") as f:\n",
    "#         key = int(in_f.split(\"_\")[1].rstrip(\".txt\"))\n",
    "#         speeches[key] = f.read()\n",
    "#         del key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/timliu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from collections import namedtuple\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(df_train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>helpfulness_cat</th>\n",
       "      <th>imdb_user_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10303</th>\n",
       "      <td>1.0</td>\n",
       "      <td>When I see reviews like this I get really susp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2175</th>\n",
       "      <td>1.0</td>\n",
       "      <td>There aren't many Tennis movies so when one do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5682</th>\n",
       "      <td>0.0</td>\n",
       "      <td>This movie makes you cry and smile. It shows t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Though in a Medieval setting, the two main cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7281</th>\n",
       "      <td>1.0</td>\n",
       "      <td>As far as I know this is the first time the il...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>1.0</td>\n",
       "      <td>There is one thing you have to know before eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>1.0</td>\n",
       "      <td>What a bitter disappointment! In order to expl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Usually I write positive reviews but for this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I first watched this film in Japanese with a 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I enjoyed this movie very much. I laughed and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7528 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       helpfulness_cat                                   imdb_user_review\n",
       "10303              1.0  When I see reviews like this I get really susp...\n",
       "2175               1.0  There aren't many Tennis movies so when one do...\n",
       "5682               0.0  This movie makes you cry and smile. It shows t...\n",
       "2540               0.0  Though in a Medieval setting, the two main cha...\n",
       "7281               1.0  As far as I know this is the first time the il...\n",
       "...                ...                                                ...\n",
       "5734               1.0  There is one thing you have to know before eve...\n",
       "5191               1.0  What a bitter disappointment! In order to expl...\n",
       "5390               1.0  Usually I write positive reviews but for this ...\n",
       "860                1.0  I first watched this film in Japanese with a 1...\n",
       "7270               1.0  I enjoyed this movie very much. I laughed and ...\n",
       "\n",
       "[7528 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>helpfulness_cat</th>\n",
       "      <th>imdb_user_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9043</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I got quite excited at the beginning when the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Hard to believe that so many people seem to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>1.0</td>\n",
       "      <td>If you like those 3 or 5 minute GoPro or Red B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10720</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Totally sappy, cheesy, silly, but sometimes yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Whip-smart 1-2-3 dialogue and film-editing, te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6399</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Don't expect a masterpiece just a very enjoyab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5936</th>\n",
       "      <td>1.0</td>\n",
       "      <td>The Actors were magnificent, fit their charact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4770</th>\n",
       "      <td>0.0</td>\n",
       "      <td>As I've said before, you need to have a great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1.0</td>\n",
       "      <td>A gang called the 'Warriors,' exhibits qualiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>1.0</td>\n",
       "      <td>The Thirteenth Floor is a thoughtful and engag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3227 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       helpfulness_cat                                   imdb_user_review\n",
       "9043               1.0  I got quite excited at the beginning when the ...\n",
       "1501               0.0  Hard to believe that so many people seem to be...\n",
       "6044               1.0  If you like those 3 or 5 minute GoPro or Red B...\n",
       "10720              1.0  Totally sappy, cheesy, silly, but sometimes yo...\n",
       "10196              1.0  Whip-smart 1-2-3 dialogue and film-editing, te...\n",
       "...                ...                                                ...\n",
       "6399               1.0  Don't expect a masterpiece just a very enjoyab...\n",
       "5936               1.0  The Actors were magnificent, fit their charact...\n",
       "4770               0.0  As I've said before, you need to have a great ...\n",
       "564                1.0  A gang called the 'Warriors,' exhibits qualiti...\n",
       "1582               1.0  The Thirteenth Floor is a thoughtful and engag...\n",
       "\n",
       "[3227 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text\n",
    "def process_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    #text = text.str\n",
    "    text = re.sub(r'\\$\\w*', '', text)\n",
    "    text = re.sub(r'^RT[\\s]+', '', text)\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n",
    "    text_tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    text_clean = []\n",
    "    for word in text_tokens:\n",
    "        if (word not in stopwords_english and  \n",
    "                word not in string.punctuation): \n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            text_clean.append(stem_word)\n",
    "            \n",
    "    return text_clean\n",
    "    #text_clean_2 = ' '.join(text_clean)\n",
    "\n",
    "    #return text_clean_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Doc2Vec: If dm=0, distributed bag of words (PV-DBOW) is used; if dm=1,‘distributed memory’ (PV-DM) is used.\n",
    "# Building TaggedDocument\n",
    "def Tagged_Document(df):\n",
    "  doc_tagged = df.apply(lambda r: TaggedDocument(words=process_text(r['imdb_user_review']), tags=[r['helpfulness_cat']]), axis=1)\n",
    "  return doc_tagged\n",
    "\n",
    "# Building the Final Vector Feature for the Classifier\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents]) # , steps=20c\n",
    "    return targets, regressors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- TaggedDocument\n",
    "train_tagged = Tagged_Document(train)\n",
    "valid_tagged = Tagged_Document(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10303    ([see, review, like, get, realli, suspici, leg...\n",
       "2175     ([mani, tenni, movi, one, come, tenni, fan, qu...\n",
       "5682     ([movi, make, cri, smile, show, hell, war, alw...\n",
       "2540     ([though, mediev, set, two, main, charact, tal...\n",
       "7281     ([far, know, first, time, illustri, studio, gh...\n",
       "                               ...                        \n",
       "5734     ([one, thing, know, even, read, movi, summari,...\n",
       "5191     ([bitter, disappoint, order, explain, miss, fi...\n",
       "5390     ([usual, write, posit, review, movi, specif, w...\n",
       "860      ([first, watch, film, japanes, 12, year, old, ...\n",
       "7270     ([enjoy, movi, much, laugh, cri, time, stori, ...\n",
       "Length: 7528, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tagged[10303][0])\n",
    "len(train_tagged[2175][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tagged[10303][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### log without pipeline\n",
    "# logreg = LogisticRegression(n_jobs=1, C=1e5, max_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### pipeline\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# pipe = make_pipeline(StandardScaler(), LogisticRegression(n_jobs=1, C=1e5, max_iter = 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bow, tagged, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7528/7528 [00:00<00:00, 2237279.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# first model\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7528/7528 [00:00<00:00, 3397323.06it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3730031.96it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3874674.26it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3830953.71it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3792303.69it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3649412.91it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3970664.05it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3470512.26it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3246090.32it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3835141.57it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3920864.34it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3940928.67it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3821680.04it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3916487.29it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3674042.41it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3957723.80it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3767866.41it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3955740.48it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3922812.84it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3717296.98it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3854806.56it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3842609.29it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3972162.60it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3930626.23it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3987210.57it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3953759.14it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3991242.64it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3955740.48it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3945360.55it/s]\n",
      "100%|██████████| 7528/7528 [00:00<00:00, 3970664.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# Training a doc2vec model is rather straight-forward in Gensim, \n",
    "# we initialize the model and train for 30 epochs.\n",
    "# %%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), \n",
    "                     total_examples=len(train_tagged.values), \n",
    "                     epochs=1)\n",
    "    model_dbow.alpha -= 0.002 # learning rate parameter\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10303    ([see, review, like, get, realli, suspici, leg...\n",
       "2175     ([mani, tenni, movi, one, come, tenni, fan, qu...\n",
       "5682     ([movi, make, cri, smile, show, hell, war, alw...\n",
       "2540     ([though, mediev, set, two, main, charact, tal...\n",
       "7281     ([far, know, first, time, illustri, studio, gh...\n",
       "                               ...                        \n",
       "5734     ([one, thing, know, even, read, movi, summari,...\n",
       "5191     ([bitter, disappoint, order, explain, miss, fi...\n",
       "5390     ([usual, write, posit, review, movi, specif, w...\n",
       "860      ([first, watch, film, japanes, 12, year, old, ...\n",
       "7270     ([enjoy, movi, much, laugh, cri, time, stori, ...\n",
       "Length: 7528, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_dbow, X_train_dbow = vec_for_learning(model_dbow, train_tagged)\n",
    "y_valid_dbow, X_valid_dbow = vec_for_learning(model_dbow, valid_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_dbow = np.array(y_train_dbow)\n",
    "X_train_dbow = np.array(X_train_dbow)\n",
    "y_valid_dbow = np.array(y_valid_dbow)\n",
    "X_valid_dbow = np.array(X_valid_dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7528, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_dbow)\n",
    "X_train_dbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7528,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_dbow)\n",
    "y_train_dbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_dbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.635625  ,  0.3911948 ,  0.9367416 , ...,  0.16619784,\n",
       "        -0.5801694 ,  0.38351798],\n",
       "       [ 0.51653475,  0.32893777,  0.76783705, ...,  0.13042964,\n",
       "        -0.47994652,  0.31983712],\n",
       "       [ 0.41095042,  0.24757954,  0.60110784, ...,  0.11524644,\n",
       "        -0.3906786 ,  0.25584203],\n",
       "       ...,\n",
       "       [ 0.6107675 ,  0.38100594,  0.90338135, ...,  0.15340132,\n",
       "        -0.56181955,  0.37409788],\n",
       "       [ 0.50463784,  0.31244645,  0.74751717, ...,  0.12372374,\n",
       "        -0.46651244,  0.31220007],\n",
       "       [ 0.44013646,  0.25527135,  0.6384526 , ...,  0.12704758,\n",
       "        -0.4190305 ,  0.272513  ]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using tensorflow of LSTM model to build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, SimpleRNN, BatchNormalization, LSTM, Conv1D, Embedding, GRU\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras.metrics import mean_squared_error\n",
    "from tensorflow.data import Dataset, TFRecordDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from tensorflow.keras.datasets import cifar10 \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, UpSampling2D, MaxPool2D\n",
    "from tensorflow.keras import backend as K\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "# clean sessions and set seeds\n",
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(231)\n",
    "tf.random.set_seed(631)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7528, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_dbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 15:15:30.512482: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model_LSTM = Sequential([\n",
    "    LSTM(300, return_sequences=True,),\n",
    "    #BatchNormalization(), # We tried it and it didn't make much difference\n",
    "    LSTM(20),\n",
    "    Dense(2, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "learning_rate = 0.001\n",
    "model_LSTM.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate), metrics=[\"accuracy\"])\n",
    "\n",
    "# model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 213, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential\" (type Sequential).\n    \n    Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 300)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 300), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/timliu/Documents/GitHub/NLP_courseproject/test.ipynb Cell 42'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/timliu/Documents/GitHub/NLP_courseproject/test.ipynb#ch0000041?line=0'>1</a>\u001b[0m \u001b[39m# fit the model \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/timliu/Documents/GitHub/NLP_courseproject/test.ipynb#ch0000041?line=1'>2</a>\u001b[0m history \u001b[39m=\u001b[39m model_LSTM\u001b[39m.\u001b[39;49mfit(X_train_dbow, y_train_dbow, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_valid_dbow, y_valid_dbow))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1129\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1129\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   1130\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 213, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential\" (type Sequential).\n    \n    Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 300)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 300), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "# fit the model \n",
    "history = model_LSTM.fit(X_train_dbow, y_train_dbow, epochs=10, validation_data=(X_valid_dbow, y_valid_dbow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_4\" is incompatible with the layer: expected shape=(None, 7528, 300), found shape=(None, 300)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/timliu/Documents/GitHub/NLP_courseproject/test.ipynb Cell 37'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/timliu/Documents/GitHub/NLP_courseproject/test.ipynb#ch0000037?line=0'>1</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/timliu/Documents/GitHub/NLP_courseproject/test.ipynb#ch0000037?line=1'>2</a>\u001b[0m \u001b[39m# early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience = 5, restore_best_weights=True)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/timliu/Documents/GitHub/NLP_courseproject/test.ipynb#ch0000037?line=2'>3</a>\u001b[0m log_LSTM\u001b[39m=\u001b[39m model_LSTM\u001b[39m.\u001b[39;49mfit(X_train_dbow, y_train_dbow, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/timliu/Documents/GitHub/NLP_courseproject/test.ipynb#ch0000037?line=3'>4</a>\u001b[0m                 epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/timliu/Documents/GitHub/NLP_courseproject/test.ipynb#ch0000037?line=4'>5</a>\u001b[0m                 validation_data\u001b[39m=\u001b[39;49m(X_valid_dbow, y_valid_dbow),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/timliu/Documents/GitHub/NLP_courseproject/test.ipynb#ch0000037?line=5'>6</a>\u001b[0m                 \u001b[39m# callbacks = [early_stopping_cb]\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/timliu/Documents/GitHub/NLP_courseproject/test.ipynb#ch0000037?line=6'>7</a>\u001b[0m                 )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1129\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1129\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   1130\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/anaconda3/envs/py38_tsf_env/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_4\" is incompatible with the layer: expected shape=(None, 7528, 300), found shape=(None, 300)\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience = 5, restore_best_weights=True)\n",
    "log_LSTM= model_LSTM.fit(X_train_dbow, y_train_dbow, \n",
    "                epochs=epochs,\n",
    "                validation_data=(X_valid_dbow, y_valid_dbow),\n",
    "                # callbacks = [early_stopping_cb]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "logreg.fit(X_train_dbow, y_train_dbow)\n",
    "\n",
    "# predict\n",
    "y_pred_dbow = logreg.predict(X_test_dbow)\n",
    "print(confusion_matrix(y_pred_dbow,y_test_dbow))\n",
    "print(classification_report(y_pred_dbow,y_test_dbow))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_tsf_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c955ac2759b198ced90504fafd0653ff1948d23274faaeb8b6184240be651d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
