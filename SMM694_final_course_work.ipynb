{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import seaborn as sns\n",
    "# import packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from spellchecker import SpellChecker\n",
    "from collections import Counter, OrderedDict\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# import en_core_web_lg\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import spacy \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pyLDAvis\n",
    "from tracemalloc import stop\n",
    "from langcodes import best_match\n",
    "from matplotlib import rc\n",
    "import matplotlib.pyplot as plt\n",
    "import tomotopy as tp\n",
    "from gensim.models import Phrases\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "import pickle # for spelling chcker\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "# BERT\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import random\n",
    "# from transformers import BertTokenizer,BertModel,get_linear_schedule_with_warmup \n",
    "# import nlpaug.augmenter.sentence as nas\n",
    "# import nlpaug.augmenter.char as nac\n",
    "# import nlpaug.augmenter.word as naw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overlooking the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('./NLP_dataset/train.csv')\n",
    "df_test = pd.read_csv('./NLP_dataset/test.csv')\n",
    "df_submission = pd.read_csv('./NLP_dataset/submission_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>helpfulness_cat</th>\n",
       "      <th>imdb_user_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>It is hard to find such delightful and adorabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>They dont make films like this faded haunting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I first viewed this movie in 1924 at age 6 yrs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I doubt that Id ever seen anything resembling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I was shocked to find myself riveted to this m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   helpfulness_cat                                   imdb_user_review\n",
       "0              1.0  It is hard to find such delightful and adorabl...\n",
       "1              1.0  They dont make films like this faded haunting ...\n",
       "2              1.0  I first viewed this movie in 1924 at age 6 yrs...\n",
       "3              1.0  I doubt that Id ever seen anything resembling ...\n",
       "4              1.0  I was shocked to find myself riveted to this m..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "  pattern = re.compile(r'\\s+')\n",
    "  text=text.replace('\\u202f',' ').replace('xa0',' ')\n",
    "  text=re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "  text=re.sub(r\"\\\\u00b4\",r'',text)\n",
    "  text=re.sub(pattern, ' ', text)\n",
    "  text = text.replace('?', ' ? ').replace(')', ') ').strip()\n",
    "  return text\n",
    "\n",
    "df['imdb_user_review']=df['imdb_user_review'].apply(preprocess)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,BertModel,get_linear_schedule_with_warmup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(text):\n",
    "#     return [str(token) for token in nlp(text) \n",
    "#         if not token.is_stop \n",
    "#         and not token.like_num\n",
    "#         and not token.is_punct\n",
    "#         and token.is_alpha\n",
    "#         ]\n",
    "# # from nltk.tokenize import TreebankWordTokenizer\n",
    "# # voc = {}\n",
    "# # tkns_= doc2bow(TreebankWordTokenizer().tokenize(df_train_clean['imbd_user_review'][0]), voc_=voc)\n",
    "tokenizer=BertTokenizer.from_pretrained('bert-base-cased',do_lower_case=True) # case and uncase \n",
    "# input_test=tokenizer(df['imdb_user_review'][0],padding='max_length',max_length=512,truncation=True,return_tensors='pt') # max_length 最大是512 # truncation 後面的不要了 # pt and tf 可以選\n",
    "# input_test\n",
    "def tokenize(text):\n",
    "  output=tokenizer(text,padding='max_length',max_length=512,truncation=True,return_tensors='pt')\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_abstract(text):\n",
    "  aug = nas.abst_summ.AbstSummAug(model_path='t5-base')\n",
    "  augmented_text=aug.augment(text) # augmented_text = summary\n",
    "  return augmented_text\n",
    "\n",
    "def aug_crop(text):\n",
    "  aug1=naw.RandomWordAug(action='crop',aug_p=0.5) # 隨機山句子\n",
    "  aug2=naw.SynonymAug(aug_src='wordnet') # 同義詞替換\n",
    "  aug3 = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\") # 根據上下文替換句子\n",
    "  return aug3.augment(aug2.augment(aug1.augment(text))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug=df[df['helpfulness_cat']==0].copy()\n",
    "df_aug=df_aug.reset_index(drop=True)\n",
    "\n",
    "df_aug['augmented_text1']=df_aug['imdb_user_review'].apply(aug_abstract)\n",
    "# df_aug['augmented_text2']=df_aug['imdb_user_review'].apply(aug_crop)\n",
    "\n",
    "# os.chdir('/content/drive/My Drive')\n",
    "df_aug\n",
    "# df_aug.to_csv('df_aug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "  def __init__(self,labels,texts):\n",
    "    self.labels=labels\n",
    "    self.texts=texts\n",
    "    self.tokens=self.texts.apply(tokenize)\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self,idx): # 一般如果想使用索引访问元素时，就可以在类中定义这个方法\n",
    "    token=self.tokens.iloc[idx]['input_ids'].squeeze(0)\n",
    "    attention_mask=self.tokens.iloc[idx]['attention_mask'].squeeze(0)\n",
    "    label=self.labels.iloc[idx]\n",
    "    return token,attention_mask,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df=train_test_split(df,test_size=0.2,random_state=42)\n",
    "\n",
    "train_dataset=ReviewDataset(train_df['helpfulness_cat'],train_df['imdb_user_review'])\n",
    "test_dataset=ReviewDataset(test_df['helpfulness_cat'],test_df['imdb_user_review'])\n",
    "\n",
    "batch_size=2\n",
    "train_loader=DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True,num_workers=2)\n",
    "test_loader=DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False,num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Data Balancing (0 & 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Changing into Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Benchmarking Model (with only text vector input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Selecting the best vector accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Other models & Adding features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Bagging, Boosting and Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Embedding algorithms + LSTM/GRU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "1.13.0.dev20220713\n"
     ]
    }
   ],
   "source": [
    "# initialize the console\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "# torch.cuda.is_available() # use GPU if available (m1 will use the cpu)\n",
    "print(torch.backends.mps.is_built())\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/timliu/Documents/GitHub/NLP_courseproject/SMM694_final_course_work.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/timliu/Documents/GitHub/NLP_courseproject/SMM694_final_course_work.ipynb#ch0000023?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBertclassifier\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/timliu/Documents/GitHub/NLP_courseproject/SMM694_final_course_work.ipynb#ch0000023?line=1'>2</a>\u001b[0m   \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,dropout\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/timliu/Documents/GitHub/NLP_courseproject/SMM694_final_course_work.ipynb#ch0000023?line=2'>3</a>\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Bertclassifier(nn.Module):\n",
    "  def __init__(self,dropout=0.5):\n",
    "    super().__init__()\n",
    "    self.Bertclassifier=BertModel.from_pretrained('bert-large-cased')\n",
    "    self.dropout=nn.Dropout(dropout)\n",
    "    self.linear1=nn.Linear(1024,256)\n",
    "    self.linear2=nn.Linear(256,2)\n",
    "    self.relu=nn.ReLU()\n",
    "\n",
    "  def forward(self,input_ids,attention_mask):\n",
    "    _,pooled_output=self.Bertclassifier(input_ids=input_ids,attention_mask=attention_mask,return_dict=False)\n",
    "    linear_output=self.relu(self.linear1(pooled_output))\n",
    "    return self.linear2(linear_output)\n",
    "\n",
    "model=Bertclassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(next(iter(train_loader))[0].to(device),next(iter(train_loader))[1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, weight=None, \n",
    "                 gamma=2, reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            target_tensor, \n",
    "            weight=self.weight,\n",
    "            reduction = self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Comparsion and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
